{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-26T01:31:32.316623Z",
     "iopub.status.busy": "2025-09-26T01:31:32.316309Z",
     "iopub.status.idle": "2025-09-26T01:31:34.558446Z",
     "shell.execute_reply": "2025-09-26T01:31:34.557471Z",
     "shell.execute_reply.started": "2025-09-26T01:31:32.316593Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/house-prices-advanced-regression-techniques/sample_submission.csv\n",
      "/kaggle/input/house-prices-advanced-regression-techniques/data_description.txt\n",
      "/kaggle/input/house-prices-advanced-regression-techniques/train.csv\n",
      "/kaggle/input/house-prices-advanced-regression-techniques/test.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-26T01:31:39.879119Z",
     "iopub.status.busy": "2025-09-26T01:31:39.878688Z",
     "iopub.status.idle": "2025-09-26T01:31:41.653285Z",
     "shell.execute_reply": "2025-09-26T01:31:41.652483Z",
     "shell.execute_reply.started": "2025-09-26T01:31:39.879086Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import xgboost as xgb\n",
    "import os\n",
    "\n",
    "# Set plot style\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-26T01:31:44.622168Z",
     "iopub.status.busy": "2025-09-26T01:31:44.621727Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please upload the kaggle.json file you downloaded from your Kaggle account.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-384a39cb-cf5a-4308-8931-498dbff69155\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-384a39cb-cf5a-4308-8931-498dbff69155\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Install the Kaggle library\n",
    "!pip install -q kaggle\n",
    "\n",
    "# Use Colab's file uploader\n",
    "from google.colab import files\n",
    "print(\"Please upload the kaggle.json file you downloaded from your Kaggle account.\")\n",
    "files.upload()\n",
    "\n",
    "# Create a directory for the Kaggle API configuration\n",
    "!mkdir -p ~/.kaggle\n",
    "# Move the uploaded kaggle.json to the required directory\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "# Set the correct permissions for the file\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "print(\"\\nKaggle API configured successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Download the dataset from the 'house-prices-advanced-regression-techniques' competition\n",
    "!kaggle competitions download -c house-prices-advanced-regression-techniques\n",
    "\n",
    "# Unzip the downloaded files\n",
    "!unzip -o house-prices-advanced-regression-techniques.zip\n",
    "\n",
    "print(\"\\nDataset downloaded and unzipped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Now, load the data from the unzipped CSV files\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# Set the 'Id' column as the index for consistency\n",
    "train_df.set_index('Id', inplace=True)\n",
    "test_df.set_index('Id', inplace=True)\n",
    "\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "print(f\"Testing data shape: {test_df.shape}\")\n",
    "\n",
    "print(\"Data loaded successfully:\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 6))\n",
    "sns.histplot(train_df['SalePrice'], kde=True, bins=50)\n",
    "plt.title('Distribution of SalePrice')\n",
    "plt.xlabel('Sale Price')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Skewness of SalePrice: {train_df['SalePrice'].skew()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_df['SalePrice'] = np.log1p(train_df['SalePrice'])\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.histplot(train_df['SalePrice'], kde=True, bins=50, color='green')\n",
    "plt.title('Distribution of Log-Transformed SalePrice')\n",
    "plt.xlabel('Log(Sale Price)')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Skewness of Log-Transformed SalePrice: {train_df['SalePrice'].skew()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Find the top 10 features most correlated with SalePrice\n",
    "corrmat = train_df.corr(numeric_only=True)\n",
    "top_corr_features = corrmat.nlargest(10, 'SalePrice')['SalePrice'].index\n",
    "top_corr_matrix = train_df[top_corr_features].corr()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(top_corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Matrix of Top 10 Features with SalePrice')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Combine train and test data for consistent preprocessing\n",
    "all_data = pd.concat((train_df.loc[:,:'SaleCondition'],\n",
    "                      test_df.loc[:,:'SaleCondition']))\n",
    "\n",
    "print(f\"Combined data shape: {all_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Find missing values in the current all_data\n",
    "missing_data = all_data.isnull().sum().sort_values(ascending=False)\n",
    "missing_data = missing_data[missing_data > 0]\n",
    "\n",
    "print(\"Features with missing values:\")\n",
    "print(missing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Impute numerical features with 0\n",
    "numerical_cols_to_impute_zero = ['MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'GarageCars', 'GarageArea', 'BsmtFullBath', 'BsmtHalfBath']\n",
    "for col in numerical_cols_to_impute_zero:\n",
    "    if col in all_data.columns:\n",
    "        all_data[col] = all_data[col].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Check missing values in numerical columns after imputation with 0\n",
    "all_data[numerical_cols_to_impute_zero].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Impute LotFrontage with the median of the neighborhood\n",
    "if 'LotFrontage' in all_data.columns and all_data['LotFrontage'].isnull().any():\n",
    "    all_data['LotFrontage'] = all_data.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n",
    "all_data['Alley'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "all_data['Electrical']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Impute categorical features with 'None' (for features where NA means 'no') or mode (for features where NA means missing data)\n",
    "categorical_cols_to_impute_none = ['Alley', 'Fence', 'MiscFeature', 'PoolQC', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'MasVnrType']\n",
    "for col in categorical_cols_to_impute_none:\n",
    "    if col in all_data.columns: # Check if column exists after one-hot encoding\n",
    "        all_data[col] = all_data[col].fillna('None')\n",
    "\n",
    "for col in ['Electrical', 'KitchenQual', 'Exterior1st', 'Exterior2nd', 'SaleType', 'Utilities', 'Functional', 'MSZoning']:\n",
    "    if col in all_data.columns: # Check if column exists after one-hot encoding\n",
    "        all_data[col] = all_data[col].fillna(all_data[col].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Based on the likely remaining missing values (GarageYrBlt), impute the remaining numerical features.\n",
    "# GarageYrBlt can be imputed with 0 (assuming 0 means no garage, consistent with GarageArea/Cars=0)\n",
    "if 'GarageYrBlt' in all_data.columns:\n",
    "    all_data['GarageYrBlt'] = all_data['GarageYrBlt'].fillna(0)\n",
    "\n",
    "\n",
    "print(\"\\nMissing values after all imputation:\", all_data.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create a total square footage feature\n",
    "all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\n",
    "\n",
    "# Create a total bathrooms feature\n",
    "all_data['TotalBath'] = (all_data['FullBath'] + (0.5 * all_data['HalfBath']) +\n",
    "                         all_data['BsmtFullBath'] + (0.5 * all_data['BsmtHalfBath']))\n",
    "\n",
    "# Create a feature for age of the house at sale\n",
    "all_data['Age'] = all_data['YrSold'] - all_data['YearBuilt']\n",
    "\n",
    "print(\"New features created.\")\n",
    "all_data['Neighborhood'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Convert any remaining object columns to category type for one-hot encoding\n",
    "categorical_cols = all_data.select_dtypes(include=['object']).columns\n",
    "all_data = pd.get_dummies(all_data, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "print(f\"Data shape after encoding: {all_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Separate the preprocessed data back into training and testing sets\n",
    "X = all_data[:len(train_df)]\n",
    "y = train_df['SalePrice'] # SalePrice was already log-transformed and is only in train_df\n",
    "X_test_final = all_data[len(train_df):]\n",
    "\n",
    "# Split the training data for validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_final_scaled = scaler.transform(X_test_final)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "y_pred_lr = lr.predict(X_val_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "xgbr = xgb.XGBRegressor(objective='reg:squarederror', # Corrected objective function\n",
    "                        n_estimators=1000,\n",
    "                        learning_rate=0.05,\n",
    "                        max_depth=3,\n",
    "                        min_child_weight=1,\n",
    "                        subsample=0.8,\n",
    "                        colsample_bytree=0.8,\n",
    "                        random_state=42)\n",
    "\n",
    "# XGBoost can handle NaNs, but since we've cleaned the data, we can use the scaled data as well if preferred.\n",
    "# However, XGBoost generally doesn't require scaling. We'll use the unscaled data as it's a tree-based model.\n",
    "xgbr.fit(X_train, y_train)\n",
    "y_pred_xgb = xgbr.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"--- {model_name} Performance ---\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"MAE:  {mae:.4f}\")\n",
    "    print(f\"R-squared: {r2:.4f}\\n\")\n",
    "\n",
    "# evaluate_model(y_val, y_pred_lr, \"Linear Regression\") # Commenting out Linear Regression evaluation as it failed\n",
    "evaluate_model(y_val, y_pred_xgb, \"XGBoost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Make predictions on the final test set using the best model (XGBoost)\n",
    "# Use the unscaled test data for XGBoost prediction\n",
    "final_predictions_log = xgbr.predict(X_test_final)\n",
    "\n",
    "# IMPORTANT: We must reverse the log transformation to get the predictions back on the original price scale\n",
    "final_predictions = np.expm1(final_predictions_log)\n",
    "\n",
    "# Create the submission DataFrame\n",
    "submission = pd.DataFrame({'Id': test_df.index, 'SalePrice': final_predictions})\n",
    "\n",
    "# Save to csv\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"Submission file 'submission.csv' created successfully.\")\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"Missing values in X_train_scaled:\", np.isnan(X_train_scaled).sum())\n",
    "print(\"Missing values in X_val_scaled:\", np.isnan(X_val_scaled).sum())\n",
    "print(\"Infinite values in X_train_scaled:\", np.isinf(X_train_scaled).sum())\n",
    "print(\"Infinite values in X_val_scaled:\", np.isinf(X_val_scaled).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m## Hyperparameter Tuning\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GridSearchCV\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxgboost\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxgb\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Define the parameter grid\u001b[39;00m\n\u001b[32m      6\u001b[39m param_grid = {\n\u001b[32m      7\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mn_estimators\u001b[39m\u001b[33m'\u001b[39m: [\u001b[32m500\u001b[39m, \u001b[32m1000\u001b[39m],\n\u001b[32m      8\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mlearning_rate\u001b[39m\u001b[33m'\u001b[39m: [\u001b[32m0.01\u001b[39m, \u001b[32m0.05\u001b[39m, \u001b[32m0.1\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m     11\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mcolsample_bytree\u001b[39m\u001b[33m'\u001b[39m: [\u001b[32m0.8\u001b[39m, \u001b[32m1.0\u001b[39m]\n\u001b[32m     12\u001b[39m }\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "\n",
    "## Hyperparameter Tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [500, 1000],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "xgbr = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "\n",
    "# GridSearchCV setup\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgbr,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit on training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters found:\", grid_search.best_params_)\n",
    "print(\"Best RMSE score:\", -grid_search.best_score_)\n",
    "\n",
    "# Use the best estimator for predictions\n",
    "best_xgbr = grid_search.best_estimator_\n",
    "y_pred_xgb_tuned = best_xgbr.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 868283,
     "sourceId": 5407,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
